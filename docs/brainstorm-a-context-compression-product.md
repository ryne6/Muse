# 上下文压缩 — 产品视角方案

> 头脑风暴 A 角色输出
> 日期: 2026-02-27

---

## 0. 现状分析

阅读代码后确认的关键事实：

- `chatStore.sendMessage()` 将 `conversation.messages` 全量发送给 AI API，无任何截断（chatStore.ts:274-288）
- `ContextIndicator` 已实现三档颜色（灰 <=70%、橙 70-90%、红 >90%），但仅做展示，无任何联动动作
- `Message` 类型目前无 `compressed` / `summaryOf` 字段（conversation.ts:3-15）
- Memory 系统已有自动提取机制（每 5 轮 fire-and-forget），但与上下文压缩完全独立
- 设计文档 `design-context-management.md` 有初步技术方案但未实现，且缺少产品层面的用户体验设计
- settingsStore 已有 `memoryEnabled` 开关模式，可作为压缩开关的参考范式

---

## 1. 触发时机

### 推荐方案：自动为主 + 手动为辅

**自动触发（P0）：**
- 时机：在 `sendMessage()` 发送 API 请求**之前**检测
- 条件：`lastInputTokens / contextLength >= threshold`（默认阈值 0.8）
- 数据来源：上一条 assistant 消息的 `inputTokens`（已有字段）+ 当前模型的 `contextLength`
- 关键决策：压缩在发送前**同步阻塞**执行，不是 fire-and-forget。原因是如果不压缩就发送，可能直接超限报错

**手动触发（P1）：**
- 在 ContextIndicator 上添加点击交互：点击弹出 popover，显示"压缩早期对话"按钮
- 斜杠命令 `/compress` 手动触发当前对话的压缩
- 适用场景：用户预判对话会很长，想提前腾出空间

### 不推荐的方案

- 纯手动触发：用户不会记得去压缩，等到报错才发现已经太晚
- 每条消息都检测：浪费计算，大部分对话不会触及阈值
- 后台定时检测：增加复杂度，且时机不可控

### 阈值设计

| 阈值 | 行为 |
|------|------|
| 70% | ContextIndicator 变橙色（已实现，仅视觉提示） |
| 80% | **触发自动压缩**（默认阈值，可配置） |
| 90% | ContextIndicator 变红 + 进度条（已实现） |
| 95%+ | 如果压缩后仍超限，显示警告 toast："对话接近上限，建议开启新对话" |

为什么默认 80% 而不是 90%：压缩本身需要一次 AI 调用消耗 token，留 20% 余量确保压缩请求本身不会超限。

---

## 2. 用户感知

### 压缩过程中的体验

**推荐方案：轻量级内联提示**

压缩发生在用户点击发送之后、AI 开始回复之前。用户体验流程：

```
用户点击发送
  → 输入框显示 "正在优化对话上下文..." (替代常规的 loading 状态)
  → 压缩完成（通常 2-5 秒）
  → 正常开始流式回复
```

具体 UI 表现：
- 复用现有的 `isLoading` 状态，不新增 loading 状态变量
- 在 assistant 消息占位区域显示："正在压缩早期对话以优化上下文..."
- 压缩完成后无缝切换为正常的流式输出
- 不使用全屏 loading 或 modal，保持轻量

**为什么不用 toast：** toast 容易被忽略，且压缩是阻塞操作，用户需要知道"为什么 AI 还没开始回复"。

### 压缩失败的体验

- 静默降级：跳过压缩，直接发送原始消息
- 如果发送也失败（超限），显示错误消息："对话过长，请开启新对话或手动删除部分消息"
- 不弹 toast 打断用户，错误信息直接显示在 assistant 消息区域（复用现有错误展示模式）

---

## 3. 压缩后展示

### 被压缩消息的 UI 处理

**推荐方案：折叠摘要卡片**

在消息列表顶部（被压缩消息的位置）显示一个折叠卡片：

```
┌─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─┐
│ ▶ 已压缩 12 条早期消息                    [展开] │
└─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─┘
```

展开后显示 AI 生成的摘要：

```
┌─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─┐
│ ▼ 已压缩 12 条早期消息                    [折叠] │
│                                                  │
│ 用户讨论了 React 组件的性能优化问题，决定使用    │
│ React.memo 包裹 MessageItem 组件，并将 ...       │
│                                                  │
│ [查看原始消息]                                   │
└─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─┘
```

### 原始消息的可访问性

**P0：** 原始消息保留在数据库（`compressed=1`），UI 默认不渲染。展开摘要卡片可看到 AI 摘要。
**P1：** 摘要卡片底部增加"查看原始消息"链接，点击后在当前位置展开原始消息（懒加载，从 DB 读取）。

### 视觉设计要点

- 摘要卡片使用虚线边框 + muted 颜色，与正常消息视觉区分
- 折叠状态下高度极小（单行），不占用有效阅读空间
- 多次压缩场景：每次压缩生成一个摘要卡片，按时间顺序排列在消息列表顶部区域
- 摘要消息的 role 设为 `assistant`，但通过 `summaryOf` 字段标识为摘要类型，MessageItem 特殊渲染

---

## 4. 设置项

### P0 最小设置

在 Settings 的现有布局中，Memory 区域下方新增"上下文管理"区块：

| 设置项 | 类型 | 默认值 | 说明 |
|--------|------|--------|------|
| 自动压缩 | Toggle | 开启 | 上下文接近上限时自动压缩早期消息 |

仅一个开关，阈值硬编码 0.8。理由：减少用户认知负担，80% 是经过权衡的合理默认值。

### P1 增强设置

| 设置项 | 类型 | 默认值 | 说明 |
|--------|------|--------|------|
| 压缩阈值 | Slider | 80% | 上下文使用量达到此比例时触发压缩 |
| 保留最近消息数 | Number | 6 | 压缩时保留最近 N 条消息不压缩 |

### 不建议暴露的设置

- 压缩使用的模型：应复用当前对话模型，不需要单独配置
- 摘要语言：应跟随对话语言自动判断
- 压缩 prompt：过于技术化，不适合暴露给普通用户

---

## 5. 与 Memory 的关系

### 推荐方案：压缩时顺带触发 Memory 提取

**理由：**
- 压缩意味着早期消息即将从 AI 的"视野"中消失，这是提取重要信息的最佳时机
- 当前 Memory 提取是每 5 轮触发一次，可能遗漏压缩区间内的重要信息
- 压缩的 AI 调用和 Memory 提取的 AI 调用可以合并为一次，减少 API 开销

**具体做法：**

```
压缩流程:
  1. 收集待压缩消息
  2. 调用 AI 生成摘要（同步，阻塞发送）
  3. 触发 Memory 提取（fire-and-forget，不阻塞）
  4. 标记原始消息为 compressed
  5. 插入摘要消息
  6. 继续正常发送
```

Memory 提取在步骤 3 异步执行，不影响压缩流程的延迟。

### 不建议的方案

- 将摘要和 Memory 提取合并为一个 prompt：两者目标不同（摘要要保留对话上下文，Memory 要提取可复用知识），合并会降低两者质量
- 压缩后不触发 Memory：会丢失早期对话中的重要信息

---

## 6. 边界情况

| 场景 | 处理方式 |
|------|----------|
| 空对话 / 只有 1 条消息 | 不触发压缩（消息数 <= KEEP_RECENT + 1 时跳过） |
| 模型无 contextLength | 不触发压缩，ContextIndicator 不显示（已有逻辑） |
| 无 inputTokens 数据 | 不触发压缩（首次对话或 API 未返回 usage） |
| 已经压缩过的对话 | 可以再次压缩。新的摘要覆盖上一次摘要之后、KEEP_RECENT 之前的消息 |
| 用户正在输入时 | 不会冲突——压缩只在 sendMessage 内部触发，此时用户已提交输入 |
| 压缩 AI 调用失败 | 静默跳过，正常发送原始消息。如果原始发送也失败，显示错误 |
| 压缩 AI 调用超时 | 设置 30 秒超时（与 Memory 提取一致），超时后跳过压缩 |
| 用户关闭自动压缩后再开启 | 下次 sendMessage 时正常检测，不追溯处理 |
| 切换模型导致 contextLength 变化 | 下次 sendMessage 时用新模型的 contextLength 重新计算比例 |
| 对话中途切换模型（context 变小） | 可能立即触发压缩，这是预期行为 |
| 并发发送（快速连续点击） | 已有 isLoading 状态防止并发，压缩在 sendMessage 内同步执行，无并发风险 |

---

## 7. 渐进式方案划分

### P0 — 最小可用版本

**目标：** 对话不会因为太长而报错，用户无需手动管理上下文。

**范围：**
1. DB schema 变更：messages 表新增 `compressed`、`summary_of` 字段
2. Message 类型扩展：新增 `compressed`、`summaryOf` 字段
3. 自动压缩逻辑：在 sendMessage 中检测阈值并触发压缩（阈值硬编码 0.8）
4. 压缩后过滤：发送 API 时过滤 `compressed=true` 的消息
5. 摘要卡片组件：`CompressedSummary.tsx`，折叠/展开摘要
6. MessageItem 集成：识别摘要消息并渲染 CompressedSummary
7. Settings 开关：一个 toggle "自动压缩"（默认开启）
8. 压缩时触发 Memory 提取（fire-and-forget）
9. 压缩过程中的 loading 提示文案

**不包含：**
- 手动触发压缩
- 阈值/保留消息数配置
- 查看原始消息
- 斜杠命令

**预估改动文件：** 约 8-10 个文件

### P1 — 增强版本

**目标：** 给高级用户更多控制权，提升压缩体验。

**范围：**
1. ContextIndicator 点击交互：popover 显示详情 + 手动压缩按钮
2. 斜杠命令 `/compress`
3. Settings 增加阈值 slider 和保留消息数配置
4. 摘要卡片"查看原始消息"功能（懒加载）
5. 压缩历史记录：在对话信息面板显示压缩次数和节省的 token 数
6. 超限警告 toast（95%+ 且压缩后仍不够）

### P2 — 远期优化

- 智能压缩策略：根据消息重要性选择性压缩（而非简单的时间顺序）
- 压缩 prompt 优化：针对代码对话场景定制摘要策略
- 多轮压缩合并：将多个摘要卡片合并为一个
- 压缩统计面板：全局查看各对话的压缩情况

---

## 8. 产品验收标准（P0）

### 核心用户故事

> 作为用户，我在一个长对话中持续编码，当上下文接近模型上限时，系统自动压缩早期消息，我可以无感知地继续对话，不会遇到"上下文超限"错误。

### 验收检查清单

- [ ] 对话 token 使用量达到 80% 时，下次发送消息前自动触发压缩
- [ ] 压缩过程中，用户看到"正在优化对话上下文..."提示
- [ ] 压缩完成后，消息列表顶部显示折叠的摘要卡片
- [ ] 摘要卡片可展开查看 AI 生成的摘要内容
- [ ] 被压缩的原始消息仍保留在数据库中
- [ ] 压缩后发送给 AI 的消息不包含已压缩的原始消息
- [ ] 压缩失败时静默降级，不影响正常发送
- [ ] Settings 中有"自动压缩"开关，关闭后不触发压缩
- [ ] 消息数不足时（<= 7 条）不触发压缩
- [ ] 模型无 contextLength 时不触发压缩
